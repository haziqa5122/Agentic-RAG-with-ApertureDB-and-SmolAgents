{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haziqa5122/Agentic-RAG-with-ApertureDB-and-SmolAgents/blob/main/ApertureDB_Agentic_RAG_with_ApertureDB_and_SmolAgents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ymNi8B3X5aKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Y6BMoENKGZ"
      },
      "outputs": [],
      "source": [
        "%pip install opendatasets openai unstructured[pdf] gradio langchain-openai aperturedb pandas langchain-community smolagents 'smolagents[litellm]' arxiv --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "iLPKGF6TOtnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "3YfVqo6jeABt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "70EoTKYGfx0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tesseract"
      ],
      "metadata": {
        "id": "YDaHhj2uPKOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr"
      ],
      "metadata": {
        "id": "3tqr0cTVcMgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! adb config create --active --from-json"
      ],
      "metadata": {
        "id": "pfqWP-jnjKV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import arxiv\n",
        "import requests\n",
        "import pandas as pd\n",
        "import opendatasets as od\n",
        "from langchain_core.documents import Document\n",
        "from unstructured.partition.auto import partition\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.chains import (\n",
        "    StuffDocumentsChain, LLMChain\n",
        ")\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.callbacks.manager import (\n",
        "    trace_as_chain_group,\n",
        ")\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Ih6OkmaZNSED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syedhamza17\n",
        "d4f0de9fd0ebb1ed71b6e3aaa7236207"
      ],
      "metadata": {
        "id": "UM-CuKjtNjkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'https://www.kaggle.com/datasets/Cornell-University/arxiv'\n",
        "od.download(dataset)"
      ],
      "metadata": {
        "id": "ZSnYRF2eNUS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_paper_details(arxiv_id):\n",
        "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[arxiv_id])))\n",
        "    paper.download_pdf( filename=f\"{arxiv_id}.pdf\")\n",
        "    return partition(f\"{arxiv_id}.pdf\")"
      ],
      "metadata": {
        "id": "JbQYSqXdNWh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "papers = []\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=5000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "sample = 5 # Arxiv has over 1.7M articles, using 20 for our application\n",
        "\n",
        "# Open the JSON file and process entries\n",
        "with open(\"arxiv/arxiv-metadata-oai-snapshot.json\", \"r\") as file:\n",
        "    for _ in range(sample):\n",
        "        line = file.readline()\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract relevant fields\n",
        "        arxiv_id = data.get(\"id\", \"\")\n",
        "\n",
        "        # Add paper details by downloading and parsing the paper\n",
        "        paper_details = \"\".join(\n",
        "            text if isinstance((text := element.text), str)\n",
        "            else \"\".join(str(part) for part in text) if isinstance(text, (list, tuple))\n",
        "            else str(text)\n",
        "            for element in fetch_paper_details(arxiv_id)\n",
        "        )\n",
        "        print(type(paper_details))\n",
        "        # Use LangChain's splitter to divide paper details into chunks\n",
        "        chunks = text_splitter.create_documents([paper_details])\n",
        "        print(len(chunks))\n",
        "        # Create a Document for each chunk\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            print(chunk,type(chunk))\n",
        "            document_id = f\"{arxiv_id}_{idx + 1}\"  # Unique ID for each chunk\n",
        "            document = Document(\n",
        "                page_content=chunk.page_content,\n",
        "                id=document_id,\n",
        "                metadata={\n",
        "                    'title': data.get(\"title\",\"\"),\n",
        "                    'authors': data.get(\"authors\", \"\"),\n",
        "                    'submitter': data.get(\"submitter\", \"\"),\n",
        "                    'abstract': data.get(\"abstract\", \"\"),\n",
        "                    'paper_content': chunk.page_content\n",
        "                }\n",
        "            )\n",
        "            papers.append(document)\n",
        "\n",
        "print(\"Processing complete. Papers saved to processed_papers.json.\")"
      ],
      "metadata": {
        "id": "y9yRn1B9ObEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"<Your-openAI-API-key>\""
      ],
      "metadata": {
        "id": "qe1Zx5qe20jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY"
      ],
      "metadata": {
        "id": "a8ovZGbc5F8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import ApertureDB\n",
        "\n",
        "embeddings = OpenAIEmbeddings(api_key  = API_KEY)\n",
        "vector_db = ApertureDB.from_documents(papers, embeddings)"
      ],
      "metadata": {
        "id": "JNBYd-d8ms-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "3u9wQt7Fs7xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_CLIENT = OpenAI(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "uCJNJVzis3Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "GENERATION_MODEL = \"gpt-4o-2024-11-20\""
      ],
      "metadata": {
        "id": "f_yQy7SZrk_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses semantic search to retrieve documents that could be relevant to answer the query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to the target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, openai_client, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedder = openai_client\n",
        "\n",
        "    def simple_retriever(self, query: str, n=5):\n",
        "      \"\"\"\n",
        "      Retrieve documents based on the given query using similarity search\n",
        "\n",
        "      Args:\n",
        "          query (str): query to pass to the DB\n",
        "          n: Number of documents to retrieve\n",
        "\n",
        "      Returns:\n",
        "          List of the retrieved documents' texts\n",
        "      \"\"\"\n",
        "\n",
        "      retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": n})\n",
        "      results = retriever.invoke(query)\n",
        "\n",
        "      return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(results)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def get_embedding(self, text: str):\n",
        "        return self.embedder.embeddings.create(\n",
        "            input=text, model=EMBEDDING_MODEL\n",
        "            ).data[0].embedding\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str)\n",
        "\n",
        "        docs = self.simple_retriever(query)\n",
        "\n",
        "        return docs\n",
        "\n",
        "retriever_tool = RetrieverTool(OPENAI_CLIENT)"
      ],
      "metadata": {
        "id": "y9jLJo4CrUTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_retriever( query: str, n=5):\n",
        "      \"\"\"\n",
        "      Retrieve documents based on the given query using similarity search\n",
        "\n",
        "      Args:\n",
        "          query (str): query to pass to the DB\n",
        "          n: Number of documents to retrieve\n",
        "\n",
        "      Returns:\n",
        "          List of the retrieved documents' texts\n",
        "      \"\"\"\n",
        "\n",
        "      retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": n})\n",
        "      results = retriever.invoke(query)\n",
        "\n",
        "      return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(results)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "def get_embedding(text: str):\n",
        "    return OPENAI_CLIENT.embeddings.create(\n",
        "        input=text, model=EMBEDDING_MODEL\n",
        "        ).data[0].embedding\n"
      ],
      "metadata": {
        "id": "6kauMZApvmyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import ToolCallingAgent, LiteLLMModel\n",
        "\n",
        "model = LiteLLMModel(model_id=GENERATION_MODEL)\n",
        "\n",
        "agent = ToolCallingAgent(tools=[retriever_tool], model=model)"
      ],
      "metadata": {
        "id": "-MlbkaxCtVc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
      ],
      "metadata": {
        "id": "BRNUWBSGxvdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Why is calculating Higgs Boson decay important?\"\n",
        "agent_output = agent.run(question)\n",
        "print(agent_output)"
      ],
      "metadata": {
        "id": "jU1UYJsMtV_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}